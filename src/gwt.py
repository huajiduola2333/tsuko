import requests
from bs4 import BeautifulSoup
import csv
import time
from datetime import date, timedelta
import os
import json
from collections import defaultdict
from ai_analyse import *


base_url = 'https://nbw.sztu.edu.cn/'
gwt_data_path = os.path.join(os.path.dirname(__file__), 'data/gwt_data.csv')
response_data_path = os.path.join(os.path.dirname(__file__), 'data/response.txt')

def get_data_from_gwt(pages = 10, days_back = 2):
    # æ ç›®å‚æ•°
    wbtreeid = '1029'
    total_pages = pages 
    date_today = date.today()
    date_from = date_today - timedelta(days=days_back)


    # CSV å­˜å‚¨
    with open(gwt_data_path, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow(['æ ‡é¢˜', 'é“¾æ¥', 'å‘å¸ƒæ—¥æœŸ'])

        # ç¿»é¡µå¾ªç¯
        for page_num in range(1, total_pages + 1):
            list_url = f'https://nbw.sztu.edu.cn/list.jsp?totalpage=603&PAGENUM={page_num}&urltype=tree.TreeTempUrl&wbtreeid={wbtreeid}'
            print(f'æ­£åœ¨æŠ“å–ç¬¬ {page_num} é¡µ: {list_url}')

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
            }
            response = requests.get(list_url, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')

            # æå–æ¯ä¸ªli
            for li in soup.select('li.clearfix'):
                title_tag = li.select_one('div.width04 a')
                if not title_tag:
                    continue  

                title = title_tag.get('title', '').strip()
                link = title_tag.get('href', '').strip()
                full_link = base_url + link if not link.startswith('http') else link

                # æå–æ—¥æœŸ
                date_div = li.select_one('div.width06')
                date_text = date_div.text.strip() if date_div else ''
                date_obj = date.fromisoformat(date_text)
                reach_date_range = False
                if date_obj < date_from:
                    reach_date_range = True
                    break

                writer.writerow([title, full_link, date_text])
            if reach_date_range:
                break

            time.sleep(1) 
    
def get_data_stored(delta = 2):
    date_today = date.today()


    date_from = date_today - timedelta(delta)

    result = []
    with open(gwt_data_path, 'r', newline='', encoding='utf-8-sig') as f:
       data = csv.DictReader(f)
       for post in data:
            date_post = date.fromisoformat(post['å‘å¸ƒæ—¥æœŸ'])
            if date_post >= date_from and date_post <= date_today:
                result.append(post)
                continue
            if date_post < date_from:
                break
    
    return result
            
def analyze(data):
    with open('prompt.txt', 'r') as f:
        prompt = f.read()
    response = ai_response(prompt, data)
    return response

def output():
    file_path = os.path.join(os.path.dirname(__file__), 'response.txt')
    
    try:
        with open(file_path, "r", encoding="utf-8-sig") as f: 
            json_str = f.read().strip()
            if json_str.startswith("```json"):
                json_str = json_str[7:]
            if json_str.endswith("```"):
                json_str = json_str[:-3]
            json_str = json_str.strip()
            data = json.loads(json_str)
        
    except FileNotFoundError:
        print(f"Error: The file '{file_path}' was not found.")
        return
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON from '{file_path}': {e}")
        return

    category_map = {
        1: "æ•™å­¦ä¸å­¦ç”Ÿç®¡ç†",
        2: "ç§‘ç ”ä¸é¡¹ç›®ç”³æŠ¥",
        3: "ä¼šè®®ä¸åŸ¹è®­æ´»åŠ¨",
        4: "å…šå»ºä¸è¡Œæ”¿äº‹åŠ¡",
        5: "é€šçŸ¥å…¬å‘Šä¸åå‹¤ä¿éšœ",
        6: "å›½é™…ä¸æ ¡å¤–äº¤æµåˆä½œ"
    }

    audience_map = {
        1: "æœ¬ç§‘ç”Ÿ",
        2: "ç ”ç©¶ç”Ÿ",
        3: "ç•™å­¦ç”Ÿ",
        4: "å…¨ä½“å­¦ç”Ÿ",
        5: "æ•™å¸ˆ",
        6: "å…¨ä½“å¸ˆç”Ÿ"
    }

    if "message" in data and "summary" in data:
        print(data["message"])
        print(data["summary"])
        print()
    else:
        print("Error: JSON data does not contain 'message' or 'summary' keys.")
        return

    # åˆ†ç»„å±•ç¤º
    if "notices" in data and isinstance(data["notices"], list):
        grouped = defaultdict(list)
        for notice in data["notices"]:
            category_name = category_map.get(notice.get("category"), "æœªçŸ¥åˆ†ç±»")
            grouped[category_name].append(notice)

        for category, notices_list in grouped.items():
            print(f"\nğŸ“Œ {category}ï¼š")
            for n in notices_list:
                audience = audience_map.get(n.get("audience"), "æœªçŸ¥å¯¹è±¡")
                print(f"- [{n.get('date', 'æœªçŸ¥æ—¥æœŸ')}] {n.get('title', 'æ— æ ‡é¢˜')}ï¼ˆå¯¹è±¡ï¼š{audience}ï¼‰")
                print(f"  ğŸ”— {n.get('link', 'æ— é“¾æ¥')}")
    else:
        print("Error: JSON data does not contain a 'notices' list or it's not a list.")
        # print(f"Notices data: {data.get('notices')}")
