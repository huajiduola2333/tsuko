import requests
from bs4 import BeautifulSoup
import csv
import time
from datetime import date, timedelta
from pathlib import Path
import json
from collections import defaultdict
from ai_analyse import *
import pandas as pd


PROJECT_ROOT = Path(__file__).resolve().parent.parent
DATA_DIR = PROJECT_ROOT / 'data'

base_url = 'https://nbw.sztu.edu.cn/'
gwt_data_path = DATA_DIR / 'gwt_data.csv'
response_data_path = DATA_DIR / 'response.txt'

def get_data_from_gwt(max_pages = 1):
    wbtreeid = '1029'
    total_pages = max_pages
    date_today = date.today()
    # date_from_config = date_today - timedelta(days=days_back)

    last_link = None
    last_date_obj = None
    existing_df = pd.DataFrame(columns=['æ ‡é¢˜', 'é“¾æ¥', 'å‘å¸ƒæ—¥æœŸ'])

    if gwt_data_path.exists() and gwt_data_path.stat().st_size > 0:
        try:
            current_data_df = pd.read_csv(gwt_data_path, dtype={'é“¾æ¥': str, 'å‘å¸ƒæ—¥æœŸ': str})
            if not current_data_df.empty:
                existing_df = current_data_df
                last_link = existing_df.iloc[0]['é“¾æ¥']
                try:
                    last_date_obj = date.fromisoformat(existing_df.iloc[0]['å‘å¸ƒæ—¥æœŸ'])
                except ValueError:
                    print(f"Warning: Could not parse date from existing CSV's first row: {existing_df.iloc[0]['å‘å¸ƒæ—¥æœŸ']}")
                    last_date_obj = None
        except pd.errors.EmptyDataError:
            print(f"Info: {gwt_data_path} is empty or contains only headers.")
        except Exception as e:
            print(f"Error reading existing CSV {gwt_data_path}: {e}")

    new_rows_list = []
    stop_scraping = False

    for page_num in range(1, total_pages + 1):
        if stop_scraping:
            break
        list_url = f'https://nbw.sztu.edu.cn/list.jsp?totalpage=603&PAGENUM={page_num}&urltype=tree.TreeTempUrl&wbtreeid={wbtreeid}'
        print(f'æ­£åœ¨æŠ“å–ç¬¬ {page_num} é¡µ: {list_url}')

        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        try:
            response = requests.get(list_url, headers=headers, timeout=10)
            response.raise_for_status()
        except requests.RequestException as e:
            print(f"Error fetching page {page_num}: {e}")
            continue # Skip to next page

        soup = BeautifulSoup(response.text, 'html.parser')

        for li in soup.select('li.clearfix'):
            title_tag = li.select_one('div.width04 a')
            if not title_tag:
                continue

            title = title_tag.get('title', '').strip()
            link_path = title_tag.get('href', '').strip()
            full_link = base_url + link_path if not link_path.startswith('http') else link_path

            date_div = li.select_one('div.width06')
            date_text = date_div.text.strip() if date_div else ''
            
            try:
                current_item_date_obj = date.fromisoformat(date_text)
            except ValueError:
                print(f"Warning: Could not parse date '{date_text}' for item '{title}'. Skipping.")
                continue

            if full_link == last_link:
                print(f"è¾¾åˆ°ä¸Šæ¬¡çˆ¬åˆ°çš„è®°å½•å¼€å¤´: {full_link}")
                stop_scraping = True
                break
            
            if last_date_obj and current_item_date_obj < last_date_obj:
                print(f"å½“å‰è®°å½•æ—¥æœŸ {current_item_date_obj} æ—©äºä¸Šæ¬¡æœ€æ–°è®°å½•æ—¥æœŸ {last_date_obj}ã€‚")
                stop_scraping = True
                break

            # if current_item_date_obj < date_from_config:
            #     print(f"å½“å‰è®°å½•æ—¥æœŸ {current_item_date_obj} æ—©äºè®¾å®šçš„æˆªæ­¢æ—¥æœŸ {date_from_config} ({days_back} å¤©å‰)ã€‚")
            #     stop_scraping = True
            #     break
            
            new_rows_list.append({'æ ‡é¢˜': title, 'é“¾æ¥': full_link, 'å‘å¸ƒæ—¥æœŸ': date_text})
        
        if stop_scraping:
            break
        time.sleep(1)
    
    if not new_rows_list:
        print("æ²¡æœ‰çˆ¬å–åˆ°æ–°æ•°æ®ã€‚")
        # Ensure file exists with headers if it was initially empty and no new data
        if existing_df.empty: # Covers file not existing or being empty initially
             final_df_to_save = pd.DataFrame(columns=['æ ‡é¢˜', 'é“¾æ¥', 'å‘å¸ƒæ—¥æœŸ'])
        else: # existing_df has data, but no new data, so save existing_df (e.g. to ensure sort/unique)
             final_df_to_save = existing_df
    else:
        new_df = pd.DataFrame(new_rows_list)
        # New data is scraped newest first. Concatenate new data before existing.
        combined_df = pd.concat([new_df, existing_df], ignore_index=True)
        final_df_to_save = combined_df

    # Process the DataFrame to be saved (either combined or just existing/new)
    if not final_df_to_save.empty:
        # Remove duplicates, keeping the first occurrence (which is the newest from concat or scrape)
        final_df_to_save = final_df_to_save.drop_duplicates(subset=['é“¾æ¥'], keep='first')
        
        # Sort by 'å‘å¸ƒæ—¥æœŸ' descending (newest first)
        try:
            final_df_to_save['temp_date_col'] = pd.to_datetime(final_df_to_save['å‘å¸ƒæ—¥æœŸ'])
            final_df_to_save = final_df_to_save.sort_values(by='temp_date_col', ascending=False).drop(columns=['temp_date_col'])
        except Exception as e:
            print(f"è­¦å‘Š: ç”±äºè§£æé”™è¯¯ï¼Œæ— æ³•æŒ‰æ—¥æœŸæ’åº: {e}ã€‚æ•°æ®æœªæŒ‰æ—¥æœŸæ’åºä¿å­˜ã€‚")
    elif final_df_to_save.columns.empty: # Ensure columns if df is completely empty
        final_df_to_save = pd.DataFrame(columns=['æ ‡é¢˜', 'é“¾æ¥', 'å‘å¸ƒæ—¥æœŸ'])


    final_df_to_save.to_csv(gwt_data_path, index=False, encoding='utf-8-sig')
    
    if new_rows_list:
        print(f"ä¿å­˜äº† {len(new_rows_list)} æ¡æ–°è®°å½•ã€‚CSVä¸­æ€»è®°å½•æ•°: {len(final_df_to_save)}.")
    elif not existing_df.equals(final_df_to_save):
         print(f"æ•°æ®å·²é‡æ–°å¤„ç† (ä¾‹å¦‚æ’åº/å»é‡)ã€‚CSVä¸­æ€»è®°å½•æ•°: {len(final_df_to_save)}.")
    else:
        print(f"CSVæ–‡ä»¶æœªå‘ç”Ÿå˜åŒ–ã€‚æ€»è®°å½•æ•°: {len(final_df_to_save)}.")

def get_data_stored(delta = 2, date_from = date.today()):
    date_from_filter = date_from - timedelta(days=delta)
    result = []

    if not gwt_data_path.exists() or gwt_data_path.stat().st_size == 0:
        print(f"Info: {gwt_data_path} ä¸å­˜åœ¨æˆ–ä¸ºç©ºã€‚")
        return result

    try:
        df = pd.read_csv(gwt_data_path, dtype={'é“¾æ¥': str, 'å‘å¸ƒæ—¥æœŸ': str})
        if df.empty:
            return result

        if 'å‘å¸ƒæ—¥æœŸ' in df.columns:
            # Convert 'å‘å¸ƒæ—¥æœŸ' to actual date objects for comparison
            # NaT for errors, then filter out NaT before comparison
            df['parsed_date'] = pd.to_datetime(df['å‘å¸ƒæ—¥æœŸ'], errors='coerce').dt.date
            
            # Filter out rows where date parsing failed
            valid_dates_df = df.dropna(subset=['parsed_date'])
            
            mask = (valid_dates_df['parsed_date'] >= date_from_filter) & \
                   (valid_dates_df['parsed_date'] <= date_from)
            filtered_df = valid_dates_df[mask]
            
            result = filtered_df[['æ ‡é¢˜', 'é“¾æ¥', 'å‘å¸ƒæ—¥æœŸ']].to_dict('records')
        else:
            print("è­¦å‘Š: CSVæ–‡ä»¶ä¸­æœªæ‰¾åˆ° 'å‘å¸ƒæ—¥æœŸ' åˆ—ã€‚æ— æ³•æŒ‰æ—¥æœŸç­›é€‰ã€‚")

    except pd.errors.EmptyDataError:
        print(f"Info: {gwt_data_path} ä¸ºç©ºã€‚æ— æ³•è·å–æ•°æ®ã€‚")
    except Exception as e:
        print(f"åœ¨ get_data_stored ä¸­è¯»å–æˆ–å¤„ç† {gwt_data_path} æ—¶å‡ºé”™: {e}")
    
    return result
            
def analyze(data):
    with open('prompt.txt', 'r') as f:
        prompt = f.read()
    response = ai_response(prompt, data)
    return response

def output(response):
    file_path = response_data_path
    
    # try:
    #     with open(file_path, "r", encoding="utf-8-sig") as f: 
    #         json_str = f.read().strip()
    #         if json_str.startswith("```json"):
    #             json_str = json_str[7:]
    #         if json_str.endswith("```"):
    #             json_str = json_str[:-3]
    #         json_str = json_str.strip()
    #         data = json.loads(json_str)
        
    # except FileNotFoundError:
    #     print(f"Error: The file '{file_path}' was not found.")
    #     return
    # except json.JSONDecodeError as e:
    #     print(f"Error decoding JSON from '{file_path}': {e}")
    #     return
    
    json_str = response.strip()
    if json_str.startswith("```json"):
        json_str = json_str[7:]
    if json_str.endswith("```"):
        json_str = json_str[:-3]
    json_str = json_str.strip()
    data = json.loads(json_str)

    category_map = {
        1: "æ•™å­¦ä¸å­¦ç”Ÿç®¡ç†",
        2: "ç§‘ç ”ä¸é¡¹ç›®ç”³æŠ¥",
        3: "ä¼šè®®ä¸åŸ¹è®­æ´»åŠ¨",
        4: "å…šå»ºä¸è¡Œæ”¿äº‹åŠ¡",
        5: "é€šçŸ¥å…¬å‘Šä¸åå‹¤ä¿éšœ",
        6: "å›½é™…ä¸æ ¡å¤–äº¤æµåˆä½œ"
    }

    audience_map = {
        1: "æœ¬ç§‘ç”Ÿ",
        2: "ç ”ç©¶ç”Ÿ",
        3: "ç•™å­¦ç”Ÿ",
        4: "å…¨ä½“å­¦ç”Ÿ",
        5: "æ•™å¸ˆ",
        6: "å…¨ä½“å¸ˆç”Ÿ"
    }

    if "message" in data and "summary" in data:
        print(data["message"])
        print(data["summary"])
        print()
    else:
        print("Error: JSON data does not contain 'message' or 'summary' keys.")
        return

    # åˆ†ç»„å±•ç¤º
    if "notices" in data and isinstance(data["notices"], list):
        grouped = defaultdict(list)
        for notice in data["notices"]:
            category_name = category_map.get(notice.get("category"), "æœªçŸ¥åˆ†ç±»")
            grouped[category_name].append(notice)

        for category, notices_list in grouped.items():
            print(f"\nğŸ“Œ {category}ï¼š")
            for n in notices_list:
                audience = audience_map.get(n.get("audience"), "æœªçŸ¥å¯¹è±¡")
                print(f"- [{n.get('date', 'æœªçŸ¥æ—¥æœŸ')}] {n.get('title', 'æ— æ ‡é¢˜')}ï¼ˆå¯¹è±¡ï¼š{audience}ï¼‰")
                print(f"  ğŸ”— {n.get('link', 'æ— é“¾æ¥')}")
    else:
        print("Error: JSON data does not contain a 'notices' list or it's not a list.")
        # print(f"Notices data: {data.get('notices')}")
